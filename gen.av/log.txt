Workflow runner started. Python executable: D:\.comfyui\.venv\Scripts\python.exe
Working directory: D:\.comfyui\gen.av
Emptying ComfyUI input and output folders...
Successfully emptied ComfyUI input and output folders.
Starting ComfyUI backend using Windows cmd style...
Waiting for ComfyUI to become ready (polling every 15s)...
[START] Security scan
[DONE] Security scan
## ComfyUI-Manager: installing dependencies done.
** ComfyUI startup time: 2026-01-18 19:41:21.610
** Platform: Windows
** Python version: 3.12.5 (tags/v3.12.5:ff3bc82, Aug  6 2024, 20:45:27) [MSC v.1940 64 bit (AMD64)]
** Python executable: D:\.comfyui\.venv\Scripts\python.exe
** ComfyUI Path: D:\.comfyui\ComfyUI
** ComfyUI Base Folder Path: D:\.comfyui\ComfyUI
** User directory: D:\.comfyui\ComfyUI\user
** ComfyUI-Manager config path: D:\.comfyui\ComfyUI\user\__manager\config.ini
** Log path: D:\.comfyui\ComfyUI\user\comfyui.log
ComfyUI not ready yet (attempt 1). Retrying in 15s...

Prestartup times for custom nodes:
   2.8 seconds: D:\.comfyui\ComfyUI\custom_nodes\comfyui-manager

Checkpoint files will always be loaded safely.
Total VRAM 12227 MB, total RAM 64957 MB
pytorch version: 2.9.1+cu130
Set vram state to: NORMAL_VRAM
Device: cuda:0 NVIDIA GeForce RTX 5070 Ti Laptop GPU : cudaMallocAsync
Using async weight offloading with 16 streams
working around nvidia conv3d memory bug.
Found comfy_kitchen backend triton: {'available': True, 'disabled': True, 'unavailable_reason': None, 'capabilities': ['apply_rope', 'apply_rope1', 'dequantize_nvfp4', 'dequantize_per_tensor_fp8', 'quantize_nvfp4', 'quantize_per_tensor_fp8']}
Found comfy_kitchen backend eager: {'available': True, 'disabled': False, 'unavailable_reason': None, 'capabilities': ['apply_rope', 'apply_rope1', 'dequantize_nvfp4', 'dequantize_per_tensor_fp8', 'quantize_nvfp4', 'quantize_per_tensor_fp8', 'scaled_mm_nvfp4']}
Found comfy_kitchen backend cuda: {'available': True, 'disabled': False, 'unavailable_reason': None, 'capabilities': ['apply_rope', 'apply_rope1', 'dequantize_nvfp4', 'dequantize_per_tensor_fp8', 'quantize_nvfp4', 'quantize_per_tensor_fp8', 'scaled_mm_nvfp4']}
Using pytorch attention
Python version: 3.12.5 (tags/v3.12.5:ff3bc82, Aug  6 2024, 20:45:27) [MSC v.1940 64 bit (AMD64)]
ComfyUI version: 0.8.2
ComfyUI frontend version: 1.36.13
[Prompt Server] web root: D:\.comfyui\.venv\Lib\site-packages\comfyui_frontend_package\static
Total VRAM 12227 MB, total RAM 64957 MB
pytorch version: 2.9.1+cu130
Set vram state to: NORMAL_VRAM
Device: cuda:0 NVIDIA GeForce RTX 5070 Ti Laptop GPU : cudaMallocAsync
Using async weight offloading with 16 streams
ComfyUI-GGUF: Allowing full torch compile
### Loading: ComfyUI-Manager (V3.39.2)
[ComfyUI-Manager] network_mode: public
[ComfyUI-Manager] ComfyUI per-queue preview override detected (PR #11261). Manager's preview method feature is disabled. Use ComfyUI's --preview-method CLI option or 'Settings > Execution > Live preview method'.
### ComfyUI Revision: 4498 [2e9d5168] *DETACHED | Released on '2026-01-07'
[ComfyUI-Manager] default cache updated: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/alter-list.json
[ComfyUI-Manager] default cache updated: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/model-list.json
[ComfyUI-Manager] default cache updated: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/github-stats.json
[ComfyUI-Manager] default cache updated: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/extension-node-map.json
[ComfyUI-Manager] default cache updated: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/custom-node-list.json

[32mInitializing ControlAltAI Nodes[0m
‚úÖ ChatterboxTTS loaded from bundled package
‚úÖ ChatterboxVC loaded from bundled package
FETCH ComfyRegistry Data: 5/120
‚úÖ F5-TTS loaded from system package
üé≠ Character voices: Found 61 characters, 2 aliases
‚úÖ SRT modules loaded from system package
üîß Registering TTS Audio Suite nodes...
======================================================================
üöÄ TTS Audio Suite v4.2.1
Universal multi-engine TTS extension for ComfyUI
‚ö†Ô∏è No local ChatterBox models found - will download from Hugging Face
üí° Tip: First generation will download models (~1GB)
   Models will be saved locally for future use
‚úÖ TTS Audio Suite v4.2.1 loaded with 16 nodes:
   ‚Ä¢ ‚öôÔ∏è ChatterBox TTS Engine
   ‚Ä¢ ‚öôÔ∏è F5 TTS Engine
   ‚Ä¢ ‚öôÔ∏è RVC Engine
   ‚Ä¢ üåä Audio Wave Analyzer
   ‚Ä¢ üéôÔ∏è Voice Capture
   ‚Ä¢ üé§ TTS Text
   ‚Ä¢ üé≠ Character Voices
   ‚Ä¢ üé≠ Load RVC Character Model
   ‚Ä¢ üëÑ F5-TTS Speech Editor
   ‚Ä¢ üì∫ TTS SRT
   ‚Ä¢ üîÑ Voice Changer
   ‚Ä¢ üîß Audio Analyzer Options
   ‚Ä¢ üîß F5-TTS Edit Options
   ‚Ä¢ üîß RVC Pitch Extraction Options
   ‚Ä¢ ü§ê Noise or Vocal Removal
   ‚Ä¢ ü•™ Merge Audio
======================================================================

Import times for custom nodes:
   0.0 seconds: D:\.comfyui\ComfyUI\custom_nodes\ComfyMath
   0.0 seconds: D:\.comfyui\ComfyUI\custom_nodes\controlaltai-nodes
   0.0 seconds: D:\.comfyui\ComfyUI\custom_nodes\ComfyUI-GGUF
   0.1 seconds: D:\.comfyui\ComfyUI\custom_nodes\comfyui-kjnodes
   0.6 seconds: D:\.comfyui\ComfyUI\custom_nodes\comfyui-videohelpersuite
   0.6 seconds: D:\.comfyui\ComfyUI\custom_nodes\comfyui-manager
   0.7 seconds: D:\.comfyui\ComfyUI\custom_nodes\ComfyUI-LTXVideo
   8.1 seconds: D:\.comfyui\ComfyUI\custom_nodes\tts_audio_suite

Context impl SQLiteImpl.
Will assume non-transactional DDL.
No target revision found.
Starting server

To see the GUI go to: http://127.0.0.1:8188
ComfyUI is ready after 18.6s (attempt 2).

===== START D:\.comfyui\gen.av\scripts\2.av.py @ 2026-01-18 19:41:38 =====
Exception in callback _ProactorBasePipeTransport._call_connection_lost(None)
handle: <Handle _ProactorBasePipeTransport._call_connection_lost(None)>
Traceback (most recent call last):
  File "C:\Users\prith\.pyenv\pyenv-win\versions\3.12.5\Lib\asyncio\events.py", line 88, in _run
    self._context.run(self._callback, *self._args)
  File "C:\Users\prith\.pyenv\pyenv-win\versions\3.12.5\Lib\asyncio\proactor_events.py", line 165, in _call_connection_lost
    self._sock.shutdown(socket.SHUT_RDWR)
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host
Resumable mode enabled - checkpoint directory: D:\.comfyui\gen.av\output\tracking
No existing checkpoint found - starting fresh
üé¨ Starting AV video generation process...
üìÅ Story:     ../input/1.story.txt
üñºÔ∏è  Images:    ../../gen.image/output/scene
üé• Output:    ../output
üìã Loaded 7 scenes with dialogue from ../input/1.story.txt
üé¨ Loaded 8 master prompts
üîó Combined 7 timeline entries into 7 unique scenes
üìã scene_1.1: 6.750s (1 segments)
üìã scene_1.2: 5.700s (1 segments)
üìã scene_1.3: 5.400s (1 segments)
üìã scene_1.4: 5.400s (1 segments)
üìã scene_1.5: 4.800s (1 segments)
üìã scene_1.6: 4.350s (1 segments)
üìã scene_1.7: 4.950s (1 segments)
Processing 7 unique scenes, skipped 0
üìä Total video words: 513
üìä Total video duration: 37.4 seconds (0.6 minutes)
============================================================

üìä AV VIDEO GENERATION PROGRESS
========================================================================================================================
üîÑ Scene 1/7 - "It was a calm winter morning in Baker Street, the (6.75s) - Processing...
üìä Estimated time remaining: ~18.7m (?)
Generating video for scene: scene_1.1
Duration: 6.75s, Frames: 162
Video chunks: 3
Copied scene image to ComfyUI input: ../../ComfyUI/input\scene_1.1.png
Generating chunk: scene_1.1_1 (73 frames)
üí¨ Chunk 1 dialogue (3.04s): "It was a calm winter morning in Baker Street, the...
üé¨ Using master prompt for 1.1
üí¨ Added dialogue chunk to prompt: "It was a calm winter morning in Baker Street, the...
Full prompt: Style: anime - cinematic - The young man sits on a blue couch near a window, reading a newspaper. Su...
got prompt
Waiting for video generation completion (prompt_id: 8e4e1d9f-ecce-4b58-a546-38a72796a012)...
Missing VAE keys ['decoder.timestep_scale_multiplier', 'decoder.last_scale_shift_table', 'decoder.up_blocks.0.time_embedder.timestep_embedder.linear_1.weight', 'decoder.up_blocks.0.time_embedder.timestep_embedder.linear_1.bias', 'decoder.up_blocks.0.time_embedder.timestep_embedder.linear_2.weight', 'decoder.up_blocks.0.time_embedder.timestep_embedder.linear_2.bias', 'decoder.up_blocks.0.res_blocks.0.scale_shift_table', 'decoder.up_blocks.0.res_blocks.1.scale_shift_table', 'decoder.up_blocks.0.res_blocks.2.scale_shift_table', 'decoder.up_blocks.0.res_blocks.3.scale_shift_table', 'decoder.up_blocks.0.res_blocks.4.scale_shift_table', 'decoder.up_blocks.2.time_embedder.timestep_embedder.linear_1.weight', 'decoder.up_blocks.2.time_embedder.timestep_embedder.linear_1.bias', 'decoder.up_blocks.2.time_embedder.timestep_embedder.linear_2.weight', 'decoder.up_blocks.2.time_embedder.timestep_embedder.linear_2.bias', 'decoder.up_blocks.2.res_blocks.0.scale_shift_table', 'decoder.up_blocks.2.res_blocks.1.scale_shift_table', 'decoder.up_blocks.2.res_blocks.2.scale_shift_table', 'decoder.up_blocks.2.res_blocks.3.scale_shift_table', 'decoder.up_blocks.2.res_blocks.4.scale_shift_table', 'decoder.up_blocks.4.time_embedder.timestep_embedder.linear_1.weight', 'decoder.up_blocks.4.time_embedder.timestep_embedder.linear_1.bias', 'decoder.up_blocks.4.time_embedder.timestep_embedder.linear_2.weight', 'decoder.up_blocks.4.time_embedder.timestep_embedder.linear_2.bias', 'decoder.up_blocks.4.res_blocks.0.scale_shift_table', 'decoder.up_blocks.4.res_blocks.1.scale_shift_table', 'decoder.up_blocks.4.res_blocks.2.scale_shift_table', 'decoder.up_blocks.4.res_blocks.3.scale_shift_table', 'decoder.up_blocks.4.res_blocks.4.scale_shift_table', 'decoder.up_blocks.6.time_embedder.timestep_embedder.linear_1.weight', 'decoder.up_blocks.6.time_embedder.timestep_embedder.linear_1.bias', 'decoder.up_blocks.6.time_embedder.timestep_embedder.linear_2.weight', 'decoder.up_blocks.6.time_embedder.timestep_embedder.linear_2.bias', 'decoder.up_blocks.6.res_blocks.0.scale_shift_table', 'decoder.up_blocks.6.res_blocks.1.scale_shift_table', 'decoder.up_blocks.6.res_blocks.2.scale_shift_table', 'decoder.up_blocks.6.res_blocks.3.scale_shift_table', 'decoder.up_blocks.6.res_blocks.4.scale_shift_table', 'decoder.last_time_embedder.timestep_embedder.linear_1.weight', 'decoder.last_time_embedder.timestep_embedder.linear_1.bias', 'decoder.last_time_embedder.timestep_embedder.linear_2.weight', 'decoder.last_time_embedder.timestep_embedder.linear_2.bias']
VAE load device: cuda:0, offload device: cpu, dtype: torch.bfloat16
Requested to load VideoVAE
FETCH ComfyRegistry Data: 10/120
loaded completely; 9606.80 MB usable, 2378.23 MB loaded, full load: True
FETCH ComfyRegistry Data: 15/120
FETCH ComfyRegistry Data: 20/120
clip missing: ['gemma3_12b.logit_scale', 'gemma3_12b.transformer.model.embed_tokens.weight', 'gemma3_12b.transformer.model.layers.0.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.0.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.0.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.0.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.0.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.0.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.0.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.0.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.0.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.0.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.0.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.0.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.0.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.1.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.1.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.1.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.1.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.1.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.1.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.1.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.1.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.1.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.1.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.1.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.1.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.1.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.2.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.2.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.2.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.2.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.2.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.2.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.2.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.2.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.2.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.2.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.2.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.2.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.2.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.3.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.3.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.3.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.3.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.3.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.3.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.3.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.3.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.3.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.3.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.3.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.3.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.3.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.4.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.4.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.4.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.4.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.4.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.4.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.4.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.4.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.4.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.4.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.4.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.4.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.4.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.5.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.5.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.5.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.5.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.5.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.5.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.5.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.5.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.5.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.5.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.5.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.5.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.5.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.6.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.6.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.6.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.6.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.6.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.6.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.6.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.6.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.6.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.6.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.6.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.6.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.6.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.7.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.7.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.7.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.7.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.7.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.7.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.7.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.7.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.7.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.7.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.7.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.7.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.7.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.8.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.8.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.8.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.8.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.8.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.8.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.8.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.8.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.8.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.8.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.8.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.8.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.8.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.9.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.9.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.9.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.9.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.9.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.9.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.9.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.9.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.9.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.9.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.9.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.9.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.9.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.10.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.10.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.10.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.10.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.10.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.10.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.10.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.10.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.10.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.10.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.10.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.10.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.10.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.11.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.11.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.11.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.11.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.11.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.11.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.11.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.11.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.11.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.11.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.11.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.11.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.11.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.12.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.12.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.12.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.12.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.12.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.12.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.12.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.12.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.12.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.12.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.12.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.12.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.12.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.13.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.13.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.13.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.13.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.13.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.13.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.13.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.13.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.13.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.13.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.13.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.13.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.13.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.14.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.14.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.14.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.14.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.14.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.14.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.14.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.14.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.14.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.14.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.14.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.14.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.14.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.15.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.15.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.15.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.15.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.15.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.15.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.15.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.15.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.15.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.15.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.15.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.15.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.15.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.16.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.16.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.16.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.16.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.16.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.16.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.16.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.16.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.16.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.16.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.16.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.16.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.16.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.17.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.17.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.17.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.17.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.17.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.17.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.17.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.17.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.17.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.17.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.17.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.17.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.17.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.18.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.18.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.18.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.18.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.18.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.18.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.18.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.18.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.18.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.18.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.18.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.18.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.18.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.19.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.19.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.19.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.19.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.19.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.19.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.19.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.19.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.19.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.19.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.19.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.19.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.19.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.20.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.20.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.20.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.20.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.20.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.20.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.20.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.20.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.20.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.20.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.20.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.20.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.20.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.21.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.21.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.21.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.21.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.21.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.21.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.21.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.21.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.21.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.21.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.21.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.21.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.21.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.22.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.22.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.22.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.22.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.22.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.22.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.22.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.22.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.22.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.22.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.22.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.22.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.22.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.23.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.23.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.23.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.23.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.23.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.23.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.23.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.23.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.23.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.23.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.23.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.23.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.23.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.24.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.24.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.24.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.24.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.24.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.24.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.24.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.24.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.24.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.24.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.24.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.24.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.24.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.25.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.25.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.25.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.25.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.25.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.25.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.25.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.25.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.25.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.25.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.25.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.25.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.25.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.26.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.26.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.26.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.26.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.26.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.26.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.26.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.26.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.26.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.26.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.26.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.26.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.26.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.27.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.27.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.27.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.27.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.27.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.27.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.27.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.27.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.27.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.27.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.27.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.27.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.27.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.28.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.28.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.28.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.28.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.28.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.28.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.28.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.28.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.28.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.28.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.28.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.28.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.28.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.29.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.29.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.29.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.29.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.29.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.29.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.29.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.29.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.29.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.29.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.29.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.29.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.29.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.30.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.30.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.30.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.30.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.30.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.30.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.30.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.30.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.30.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.30.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.30.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.30.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.30.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.31.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.31.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.31.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.31.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.31.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.31.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.31.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.31.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.31.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.31.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.31.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.31.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.31.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.32.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.32.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.32.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.32.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.32.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.32.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.32.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.32.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.32.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.32.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.32.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.32.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.32.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.33.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.33.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.33.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.33.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.33.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.33.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.33.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.33.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.33.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.33.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.33.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.33.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.33.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.34.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.34.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.34.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.34.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.34.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.34.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.34.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.34.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.34.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.34.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.34.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.34.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.34.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.35.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.35.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.35.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.35.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.35.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.35.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.35.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.35.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.35.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.35.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.35.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.35.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.35.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.36.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.36.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.36.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.36.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.36.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.36.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.36.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.36.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.36.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.36.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.36.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.36.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.36.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.37.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.37.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.37.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.37.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.37.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.37.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.37.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.37.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.37.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.37.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.37.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.37.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.37.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.38.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.38.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.38.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.38.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.38.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.38.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.38.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.38.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.38.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.38.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.38.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.38.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.38.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.39.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.39.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.39.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.39.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.39.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.39.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.39.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.39.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.39.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.39.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.39.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.39.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.39.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.40.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.40.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.40.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.40.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.40.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.40.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.40.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.40.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.40.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.40.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.40.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.40.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.40.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.41.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.41.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.41.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.41.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.41.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.41.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.41.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.41.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.41.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.41.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.41.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.41.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.41.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.42.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.42.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.42.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.42.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.42.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.42.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.42.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.42.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.42.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.42.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.42.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.42.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.42.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.43.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.43.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.43.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.43.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.43.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.43.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.43.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.43.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.43.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.43.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.43.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.43.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.43.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.44.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.44.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.44.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.44.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.44.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.44.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.44.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.44.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.44.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.44.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.44.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.44.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.44.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.45.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.45.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.45.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.45.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.45.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.45.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.45.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.45.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.45.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.45.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.45.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.45.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.45.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.46.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.46.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.46.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.46.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.46.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.46.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.46.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.46.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.46.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.46.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.46.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.46.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.46.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.47.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.47.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.47.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.47.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.47.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.47.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.47.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.47.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.47.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.47.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.47.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.47.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.47.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.norm.weight', 'gemma3_12b.transformer.multi_modal_projector.mm_input_projection_weight', 'gemma3_12b.transformer.multi_modal_projector.mm_soft_emb_norm.weight', 'gemma3_12b.transformer.vision_model.embeddings.patch_embedding.weight', 'gemma3_12b.transformer.vision_model.embeddings.patch_embedding.bias', 'gemma3_12b.transformer.vision_model.embeddings.position_embedding.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.0.layer_norm1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.0.layer_norm1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.0.layer_norm2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.0.layer_norm2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.0.mlp.fc1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.0.mlp.fc1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.0.mlp.fc2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.0.mlp.fc2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.1.layer_norm1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.1.layer_norm1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.1.layer_norm2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.1.layer_norm2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.1.mlp.fc1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.1.mlp.fc1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.1.mlp.fc2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.1.mlp.fc2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.2.layer_norm1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.2.layer_norm1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.2.layer_norm2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.2.layer_norm2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.2.mlp.fc1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.2.mlp.fc1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.2.mlp.fc2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.2.mlp.fc2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.3.layer_norm1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.3.layer_norm1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.3.layer_norm2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.3.layer_norm2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.3.mlp.fc1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.3.mlp.fc1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.3.mlp.fc2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.3.mlp.fc2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.4.layer_norm1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.4.layer_norm1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.4.layer_norm2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.4.layer_norm2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.4.mlp.fc1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.4.mlp.fc1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.4.mlp.fc2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.4.mlp.fc2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.5.layer_norm1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.5.layer_norm1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.5.layer_norm2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.5.layer_norm2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.5.mlp.fc1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.5.mlp.fc1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.5.mlp.fc2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.5.mlp.fc2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.6.layer_norm1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.6.layer_norm1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.6.layer_norm2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.6.layer_norm2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.6.mlp.fc1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.6.mlp.fc1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.6.mlp.fc2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.6.mlp.fc2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.7.layer_norm1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.7.layer_norm1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.7.layer_norm2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.7.layer_norm2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.7.mlp.fc1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.7.mlp.fc1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.7.mlp.fc2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.7.mlp.fc2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.8.layer_norm1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.8.layer_norm1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.8.layer_norm2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.8.layer_norm2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.8.mlp.fc1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.8.mlp.fc1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.8.mlp.fc2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.8.mlp.fc2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.9.layer_norm1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.9.layer_norm1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.9.layer_norm2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.9.layer_norm2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.9.mlp.fc1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.9.mlp.fc1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.9.mlp.fc2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.9.mlp.fc2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.10.layer_norm1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.10.layer_norm1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.10.layer_norm2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.10.layer_norm2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.10.mlp.fc1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.10.mlp.fc1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.10.mlp.fc2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.10.mlp.fc2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.11.layer_norm1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.11.layer_norm1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.11.layer_norm2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.11.layer_norm2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.11.mlp.fc1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.11.mlp.fc1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.11.mlp.fc2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.11.mlp.fc2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.12.layer_norm1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.12.layer_norm1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.12.self_attn.q_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.12.self_attn.q_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.12.self_attn.k_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.12.self_attn.k_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.12.self_attn.v_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.12.self_attn.v_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.12.self_attn.out_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.12.self_attn.out_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.12.layer_norm2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.12.layer_norm2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.12.mlp.fc1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.12.mlp.fc1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.12.mlp.fc2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.12.mlp.fc2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.13.layer_norm1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.13.layer_norm1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.13.self_attn.q_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.13.self_attn.q_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.13.self_attn.k_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.13.self_attn.k_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.13.self_attn.v_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.13.self_attn.v_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.13.self_attn.out_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.13.self_attn.out_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.13.layer_norm2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.13.layer_norm2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.13.mlp.fc1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.13.mlp.fc1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.13.mlp.fc2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.13.mlp.fc2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.14.layer_norm1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.14.layer_norm1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.14.self_attn.q_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.14.self_attn.q_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.14.self_attn.k_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.14.self_attn.k_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.14.self_attn.v_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.14.self_attn.v_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.14.self_attn.out_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.14.self_attn.out_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.14.layer_norm2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.14.layer_norm2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.14.mlp.fc1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.14.mlp.fc1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.14.mlp.fc2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.14.mlp.fc2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.15.layer_norm1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.15.layer_norm1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.15.self_attn.q_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.15.self_attn.q_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.15.self_attn.k_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.15.self_attn.k_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.15.self_attn.v_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.15.self_attn.v_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.15.self_attn.out_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.15.self_attn.out_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.15.layer_norm2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.15.layer_norm2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.15.mlp.fc1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.15.mlp.fc1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.15.mlp.fc2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.15.mlp.fc2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.16.layer_norm1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.16.layer_norm1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.16.self_attn.q_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.16.self_attn.q_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.16.self_attn.k_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.16.self_attn.k_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.16.self_attn.v_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.16.self_attn.v_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.16.self_attn.out_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.16.self_attn.out_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.16.layer_norm2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.16.layer_norm2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.16.mlp.fc1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.16.mlp.fc1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.16.mlp.fc2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.16.mlp.fc2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.17.layer_norm1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.17.layer_norm1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.17.self_attn.q_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.17.self_attn.q_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.17.self_attn.k_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.17.self_attn.k_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.17.self_attn.v_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.17.self_attn.v_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.17.self_attn.out_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.17.self_attn.out_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.17.layer_norm2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.17.layer_norm2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.17.mlp.fc1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.17.mlp.fc1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.17.mlp.fc2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.17.mlp.fc2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.18.layer_norm1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.18.layer_norm1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.18.self_attn.q_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.18.self_attn.q_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.18.self_attn.k_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.18.self_attn.k_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.18.self_attn.v_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.18.self_attn.v_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.18.self_attn.out_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.18.self_attn.out_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.18.layer_norm2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.18.layer_norm2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.18.mlp.fc1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.18.mlp.fc1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.18.mlp.fc2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.18.mlp.fc2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.19.layer_norm1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.19.layer_norm1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.19.self_attn.q_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.19.self_attn.q_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.19.self_attn.k_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.19.self_attn.k_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.19.self_attn.v_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.19.self_attn.v_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.19.self_attn.out_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.19.self_attn.out_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.19.layer_norm2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.19.layer_norm2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.19.mlp.fc1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.19.mlp.fc1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.19.mlp.fc2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.19.mlp.fc2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.20.layer_norm1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.20.layer_norm1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.20.self_attn.q_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.20.self_attn.q_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.20.self_attn.k_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.20.self_attn.k_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.20.self_attn.v_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.20.self_attn.v_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.20.self_attn.out_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.20.self_attn.out_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.20.layer_norm2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.20.layer_norm2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.20.mlp.fc1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.20.mlp.fc1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.20.mlp.fc2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.20.mlp.fc2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.21.layer_norm1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.21.layer_norm1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.21.self_attn.q_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.21.self_attn.q_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.21.self_attn.k_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.21.self_attn.k_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.21.self_attn.v_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.21.self_attn.v_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.21.self_attn.out_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.21.self_attn.out_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.21.layer_norm2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.21.layer_norm2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.21.mlp.fc1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.21.mlp.fc1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.21.mlp.fc2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.21.mlp.fc2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.22.layer_norm1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.22.layer_norm1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.22.self_attn.q_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.22.self_attn.q_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.22.self_attn.k_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.22.self_attn.k_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.22.self_attn.v_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.22.self_attn.v_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.22.self_attn.out_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.22.self_attn.out_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.22.layer_norm2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.22.layer_norm2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.22.mlp.fc1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.22.mlp.fc1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.22.mlp.fc2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.22.mlp.fc2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.23.layer_norm1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.23.layer_norm1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.23.self_attn.q_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.23.self_attn.q_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.23.self_attn.k_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.23.self_attn.k_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.23.self_attn.v_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.23.self_attn.v_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.23.self_attn.out_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.23.self_attn.out_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.23.layer_norm2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.23.layer_norm2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.23.mlp.fc1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.23.mlp.fc1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.23.mlp.fc2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.23.mlp.fc2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.24.layer_norm1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.24.layer_norm1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.24.self_attn.q_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.24.self_attn.q_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.24.self_attn.k_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.24.self_attn.k_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.24.self_attn.v_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.24.self_attn.v_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.24.self_attn.out_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.24.self_attn.out_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.24.layer_norm2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.24.layer_norm2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.24.mlp.fc1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.24.mlp.fc1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.24.mlp.fc2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.24.mlp.fc2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.25.layer_norm1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.25.layer_norm1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.25.self_attn.q_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.25.self_attn.q_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.25.self_attn.k_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.25.self_attn.k_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.25.self_attn.v_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.25.self_attn.v_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.25.self_attn.out_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.25.self_attn.out_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.25.layer_norm2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.25.layer_norm2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.25.mlp.fc1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.25.mlp.fc1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.25.mlp.fc2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.25.mlp.fc2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.26.layer_norm1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.26.layer_norm1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.26.self_attn.q_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.26.self_attn.q_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.26.self_attn.k_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.26.self_attn.k_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.26.self_attn.v_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.26.self_attn.v_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.26.self_attn.out_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.26.self_attn.out_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.26.layer_norm2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.26.layer_norm2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.26.mlp.fc1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.26.mlp.fc1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.26.mlp.fc2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.26.mlp.fc2.bias', 'gemma3_12b.transformer.vision_model.post_layernorm.weight', 'gemma3_12b.transformer.vision_model.post_layernorm.bias']
CLIP/text encoder model load device: cuda:0, offload device: cpu, current: cpu, dtype: torch.float16
Requested to load LTXAVTEModel_
loaded partially; 9562.80 MB usable, 735.62 MB loaded, 25228.59 MB offloaded, 8827.18 MB buffer reserved, lowvram patches: 0
FETCH ComfyRegistry Data: 25/120
FETCH ComfyRegistry Data: 30/120
gguf qtypes: F32 (2140), BF16 (26), Q4_K (1008), Q6_K (336)
model weight dtype torch.bfloat16, manual cast: None
model_type FLUX
Requested to load LTXAV
FETCH ComfyRegistry Data: 35/120
FETCH ComfyRegistry Data: 40/120
FETCH ComfyRegistry Data: 45/120
FETCH ComfyRegistry Data: 50/120
loaded partially; 9541.56 MB usable, 9388.43 MB loaded, 2853.54 MB offloaded, 153.13 MB buffer reserved, lowvram patches: 0
  0%|          | 0/8 [00:00<?, ?it/s]FETCH ComfyRegistry Data: 55/120
FETCH ComfyRegistry Data: 60/120
FETCH ComfyRegistry Data: 65/120
FETCH ComfyRegistry Data: 70/120
 12%|‚ñà‚ñé        | 1/8 [00:20<02:21, 20.28s/it]FETCH ComfyRegistry Data: 75/120
FETCH ComfyRegistry Data: 80/120
 25%|‚ñà‚ñà‚ñå       | 2/8 [00:31<01:28, 14.77s/it]FETCH ComfyRegistry Data: 85/120
FETCH ComfyRegistry Data: 90/120
 38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:41<01:04, 12.95s/it]FETCH ComfyRegistry Data: 95/120
 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:52<00:48, 12.11s/it]FETCH ComfyRegistry Data: 100/120
FETCH ComfyRegistry Data: 105/120
 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [01:03<00:34, 11.64s/it]FETCH ComfyRegistry Data: 110/120
FETCH ComfyRegistry Data: 115/120
 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [01:14<00:22, 11.37s/it]FETCH ComfyRegistry Data: 120/120
FETCH ComfyRegistry Data [DONE]
[ComfyUI-Manager] default cache updated: https://api.comfy.org/nodes
FETCH DATA from: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/custom-node-list.json [DONE]
[ComfyUI-Manager] All startup tasks have been completed.
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [01:25<00:11, 11.35s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [01:36<00:00, 11.23s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [01:36<00:00, 12.09s/it]
Requested to load VideoVAE
Unloaded partially: 2398.64 MB freed, 6989.78 MB remains loaded, 177.49 MB buffer reserved, lowvram patches: 211
loaded completely; 2648.72 MB usable, 2378.23 MB loaded, full load: True
loaded partially; 9227.11 MB usable, 9073.58 MB loaded, 3168.39 MB offloaded, 153.53 MB buffer reserved, lowvram patches: 0
  0%|          | 0/3 [00:00<?, ?it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:22<00:44, 22.35s/it] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:44<00:22, 22.31s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [01:06<00:00, 22.29s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [01:06<00:00, 22.30s/it]
Requested to load VideoVAE
loaded partially; 564.00 MB usable, 0.00 MB loaded, 2378.23 MB offloaded, 1728.05 MB buffer reserved, lowvram patches: 0
Requested to load AudioVAE
loaded completely; 9541.67 MB usable, 415.20 MB loaded, full load: True
Prompt executed in 244.55 seconds
‚úÖ Copied 73 frames for scene_1.1_1
Saved video: ../output\scene_1.1_1.mp4
üíæ Checkpoint saved for chunk: scene_1.1_1
Generating chunk: scene_1.1_2 (73 frames)
üîç Frame prefix: scene_1.1_1.frame_
üîç Matching frames: 73
üîç Found last frame: scene_1.1_1.frame_073.png (frame #73)
üîÑ Using last saved frame from previous chunk as input for scene_1.1_2
üìÅ Frame stored: ../output/frames\scene_1.1_2_last_frame.png
üí¨ Chunk 2 dialogue (3.04s): I sat with my newspaper I little suspected that a ...
üé¨ Using master prompt for 1.1
üí¨ Added dialogue chunk to prompt: I sat with my newspaper I little suspected that a ...
Full prompt: Style: anime - cinematic - The young man sits on a blue couch near a window, reading a newspaper. Su...
got prompt
Waiting for video generation completion (prompt_id: a99f58e2-c16d-46c2-8dac-054a56130268)...
loaded completely; 9126.48 MB usable, 2378.23 MB loaded, full load: True
Requested to load LTXAVTEModel_
loaded partially; 9541.67 MB usable, 714.50 MB loaded, 25249.59 MB offloaded, 8827.18 MB buffer reserved, lowvram patches: 0
Requested to load LTXAV
loaded partially; 9539.56 MB usable, 9386.43 MB loaded, 2855.54 MB offloaded, 153.13 MB buffer reserved, lowvram patches: 0
  0%|          | 0/8 [00:00<?, ?it/s]Traceback (most recent call last):
  File "D:\.comfyui\gen.av\scripts\2.av.py", line 1608, in <module>
    exit(main())
         ^^^^^^
  File "D:\.comfyui\gen.av\scripts\2.av.py", line 1582, in main
    results = generator.generate_all_videos(force_regenerate=args.force, resumable_state=resumable_state)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\.comfyui\gen.av\scripts\2.av.py", line 1511, in generate_all_videos
    output_paths = self._generate_video(scene_name, description, image_path, duration, motion_data, resumable_state, dialogue, scene_description_only)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\.comfyui\gen.av\scripts\2.av.py", line 1080, in _generate_video
    time.sleep(3)  # Video generation takes longer
    ^^^^^^^^^^^^^
KeyboardInterrupt
Received signal 2. Cleaning up services...
Stopping ComfyUI backend...
ComfyUI still responding; waiting for shutdown...
ComfyUI appears stopped (after 2 checks).
