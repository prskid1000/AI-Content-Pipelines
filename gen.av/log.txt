Workflow runner started. Python executable: D:\.comfyui\.venv\Scripts\python.exe
Working directory: D:\.comfyui\gen.av
Emptying ComfyUI input and output folders...
Removed directory: D:\.comfyui\ComfyUI\input\3d
Removed file: D:\.comfyui\ComfyUI\input\scene_1.1.png
Successfully emptied ComfyUI input and output folders.
Starting ComfyUI backend using Windows cmd style...
Waiting for ComfyUI to become ready (polling every 15s)...
[START] Security scan
[DONE] Security scan
## ComfyUI-Manager: installing dependencies done.
** ComfyUI startup time: 2026-01-19 08:31:29.177
** Platform: Windows
** Python version: 3.12.5 (tags/v3.12.5:ff3bc82, Aug  6 2024, 20:45:27) [MSC v.1940 64 bit (AMD64)]
** Python executable: D:\.comfyui\.venv\Scripts\python.exe
** ComfyUI Path: D:\.comfyui\ComfyUI
** ComfyUI Base Folder Path: D:\.comfyui\ComfyUI
** User directory: D:\.comfyui\ComfyUI\user
** ComfyUI-Manager config path: D:\.comfyui\ComfyUI\user\__manager\config.ini
** Log path: D:\.comfyui\ComfyUI\user\comfyui.log
ComfyUI not ready yet (attempt 1). Retrying in 15s...

Prestartup times for custom nodes:
   3.0 seconds: D:\.comfyui\ComfyUI\custom_nodes\comfyui-manager

Checkpoint files will always be loaded safely.
Total VRAM 12227 MB, total RAM 64957 MB
pytorch version: 2.9.1+cu130
Set vram state to: NORMAL_VRAM
Device: cuda:0 NVIDIA GeForce RTX 5070 Ti Laptop GPU : cudaMallocAsync
Using async weight offloading with 16 streams
working around nvidia conv3d memory bug.
Found comfy_kitchen backend triton: {'available': True, 'disabled': True, 'unavailable_reason': None, 'capabilities': ['apply_rope', 'apply_rope1', 'dequantize_nvfp4', 'dequantize_per_tensor_fp8', 'quantize_nvfp4', 'quantize_per_tensor_fp8']}
Found comfy_kitchen backend eager: {'available': True, 'disabled': False, 'unavailable_reason': None, 'capabilities': ['apply_rope', 'apply_rope1', 'dequantize_nvfp4', 'dequantize_per_tensor_fp8', 'quantize_nvfp4', 'quantize_per_tensor_fp8', 'scaled_mm_nvfp4']}
Found comfy_kitchen backend cuda: {'available': True, 'disabled': False, 'unavailable_reason': None, 'capabilities': ['apply_rope', 'apply_rope1', 'dequantize_nvfp4', 'dequantize_per_tensor_fp8', 'quantize_nvfp4', 'quantize_per_tensor_fp8', 'scaled_mm_nvfp4']}
Using pytorch attention
Python version: 3.12.5 (tags/v3.12.5:ff3bc82, Aug  6 2024, 20:45:27) [MSC v.1940 64 bit (AMD64)]
ComfyUI version: 0.8.2
ComfyUI frontend version: 1.36.13
[Prompt Server] web root: D:\.comfyui\.venv\Lib\site-packages\comfyui_frontend_package\static
Total VRAM 12227 MB, total RAM 64957 MB
pytorch version: 2.9.1+cu130
Set vram state to: NORMAL_VRAM
Device: cuda:0 NVIDIA GeForce RTX 5070 Ti Laptop GPU : cudaMallocAsync
Using async weight offloading with 16 streams
ComfyUI-GGUF: Allowing full torch compile
### Loading: ComfyUI-Manager (V3.39.2)
[ComfyUI-Manager] network_mode: public
[ComfyUI-Manager] ComfyUI per-queue preview override detected (PR #11261). Manager's preview method feature is disabled. Use ComfyUI's --preview-method CLI option or 'Settings > Execution > Live preview method'.
### ComfyUI Revision: 4498 [2e9d5168] *DETACHED | Released on '2026-01-07'

[32mInitializing ControlAltAI Nodes[0m
[ComfyUI-Manager] default cache updated: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/alter-list.json
[ComfyUI-Manager] default cache updated: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/model-list.json
[ComfyUI-Manager] default cache updated: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/github-stats.json
[ComfyUI-Manager] default cache updated: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/extension-node-map.json
[ComfyUI-Manager] default cache updated: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/custom-node-list.json
‚úÖ ChatterboxTTS loaded from bundled package
‚úÖ ChatterboxVC loaded from bundled package
FETCH ComfyRegistry Data: 5/120
‚úÖ F5-TTS loaded from system package
üé≠ Character voices: Found 61 characters, 2 aliases
‚úÖ SRT modules loaded from system package
üîß Registering TTS Audio Suite nodes...
======================================================================
üöÄ TTS Audio Suite v4.2.1
Universal multi-engine TTS extension for ComfyUI
‚ö†Ô∏è No local ChatterBox models found - will download from Hugging Face
üí° Tip: First generation will download models (~1GB)
   Models will be saved locally for future use
‚úÖ TTS Audio Suite v4.2.1 loaded with 16 nodes:
   ‚Ä¢ ‚öôÔ∏è ChatterBox TTS Engine
   ‚Ä¢ ‚öôÔ∏è F5 TTS Engine
   ‚Ä¢ ‚öôÔ∏è RVC Engine
   ‚Ä¢ üåä Audio Wave Analyzer
   ‚Ä¢ üéôÔ∏è Voice Capture
   ‚Ä¢ üé§ TTS Text
   ‚Ä¢ üé≠ Character Voices
   ‚Ä¢ üé≠ Load RVC Character Model
   ‚Ä¢ üëÑ F5-TTS Speech Editor
   ‚Ä¢ üì∫ TTS SRT
   ‚Ä¢ üîÑ Voice Changer
   ‚Ä¢ üîß Audio Analyzer Options
   ‚Ä¢ üîß F5-TTS Edit Options
   ‚Ä¢ üîß RVC Pitch Extraction Options
   ‚Ä¢ ü§ê Noise or Vocal Removal
   ‚Ä¢ ü•™ Merge Audio
======================================================================

Import times for custom nodes:
   0.0 seconds: D:\.comfyui\ComfyUI\custom_nodes\ComfyMath
   0.0 seconds: D:\.comfyui\ComfyUI\custom_nodes\ComfyUI-GGUF
   0.0 seconds: D:\.comfyui\ComfyUI\custom_nodes\controlaltai-nodes
   0.0 seconds: D:\.comfyui\ComfyUI\custom_nodes\comfyui-kjnodes
   0.1 seconds: D:\.comfyui\ComfyUI\custom_nodes\comfyui-videohelpersuite
   0.4 seconds: D:\.comfyui\ComfyUI\custom_nodes\comfyui-manager
   0.6 seconds: D:\.comfyui\ComfyUI\custom_nodes\ComfyUI-LTXVideo
   8.1 seconds: D:\.comfyui\ComfyUI\custom_nodes\tts_audio_suite

Context impl SQLiteImpl.
Will assume non-transactional DDL.
No target revision found.
Starting server

To see the GUI go to: http://127.0.0.1:8188
ComfyUI is ready after 18.1s (attempt 2).

===== START D:\.comfyui\gen.av\scripts\2.av.py @ 2026-01-19 08:31:45 =====
Resumable mode enabled - checkpoint directory: D:\.comfyui\gen.av\output\tracking
No existing checkpoint found - starting fresh
üé¨ Starting AV video generation process...
üìÅ Story:     ../input/1.story.txt
üñºÔ∏è  Images:    ../../gen.image/output/scene
üé• Output:    ../output
üìã Loaded 7 scenes with dialogue from ../input/1.story.txt
üé¨ Loaded 8 master prompts
üîó Combined 7 timeline entries into 7 unique scenes
üìã scene_1.1: 9.000s (1 segments)
üìã scene_1.2: 7.600s (1 segments)
üìã scene_1.3: 7.200s (1 segments)
üìã scene_1.4: 7.200s (1 segments)
üìã scene_1.5: 6.400s (1 segments)
üìã scene_1.6: 5.800s (1 segments)
üìã scene_1.7: 6.600s (1 segments)
Processing 7 unique scenes, skipped 0
üìä Total video words: 513
üìä Total video duration: 49.8 seconds (0.8 minutes)
============================================================

üìä AV VIDEO GENERATION PROGRESS
========================================================================================================================
üîÑ Scene 1/7 - "It was a calm winter morning in Baker Street, the (9.00s) - Processing...
üìä Estimated time remaining: ~24.9m (?)
Generating video for scene: scene_1.1
Duration: 9.00s, Frames: 216
Video chunks: 3
Copied scene image to ComfyUI input: ../../ComfyUI/input\scene_1.1.png
Generating chunk: scene_1.1_1 (73 frames)
üí¨ Chunk 1 dialogue (3.04s, 15 words): "It was a calm winter morning in Baker Street, the...
üé¨ Using master prompt for 1.1
üí¨ Added dialogue chunk to prompt: "It was a calm winter morning in Baker Street, the...
Full prompt: Style: anime - cinematic - The young man sits on a blue couch near a window, reading a newspaper. Sunlight streams through the glass, illuminating his face and casting soft shadows across the room. He wears a white short-sleeved shirt and brown pants, with a watch on his left wrist. A small wooden table stands beside him, while green plants grow in the corner of the room. On the wall behind him hangs a framed picture. Outside the window, trees sway gently under a bright blue sky dotted with fluffy clouds. The sound of birds chirping fills the air, accompanied by the soft rustling of leaves and distant traffic noise from the street below.

Dialogue: ""It was a calm winter morning in Baker Street, the sort that carries with it"
got prompt
Waiting for video generation completion (prompt_id: 69fc76bd-0318-48b8-992e-f7aba26fa6cb)...
VAE load device: cuda:0, offload device: cpu, dtype: torch.bfloat16
Requested to load VideoVAE
loaded completely; 9606.80 MB usable, 2378.23 MB loaded, full load: True
FETCH ComfyRegistry Data: 10/120
FETCH ComfyRegistry Data: 15/120
clip missing: ['gemma3_12b.logit_scale', 'gemma3_12b.transformer.model.embed_tokens.weight', 'gemma3_12b.transformer.model.layers.0.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.0.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.0.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.0.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.0.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.0.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.0.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.0.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.0.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.0.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.0.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.0.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.0.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.1.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.1.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.1.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.1.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.1.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.1.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.1.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.1.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.1.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.1.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.1.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.1.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.1.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.2.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.2.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.2.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.2.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.2.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.2.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.2.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.2.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.2.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.2.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.2.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.2.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.2.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.3.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.3.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.3.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.3.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.3.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.3.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.3.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.3.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.3.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.3.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.3.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.3.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.3.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.4.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.4.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.4.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.4.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.4.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.4.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.4.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.4.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.4.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.4.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.4.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.4.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.4.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.5.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.5.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.5.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.5.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.5.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.5.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.5.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.5.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.5.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.5.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.5.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.5.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.5.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.6.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.6.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.6.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.6.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.6.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.6.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.6.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.6.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.6.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.6.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.6.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.6.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.6.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.7.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.7.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.7.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.7.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.7.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.7.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.7.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.7.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.7.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.7.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.7.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.7.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.7.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.8.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.8.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.8.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.8.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.8.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.8.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.8.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.8.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.8.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.8.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.8.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.8.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.8.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.9.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.9.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.9.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.9.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.9.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.9.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.9.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.9.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.9.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.9.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.9.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.9.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.9.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.10.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.10.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.10.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.10.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.10.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.10.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.10.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.10.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.10.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.10.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.10.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.10.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.10.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.11.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.11.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.11.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.11.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.11.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.11.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.11.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.11.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.11.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.11.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.11.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.11.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.11.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.12.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.12.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.12.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.12.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.12.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.12.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.12.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.12.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.12.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.12.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.12.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.12.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.12.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.13.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.13.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.13.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.13.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.13.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.13.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.13.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.13.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.13.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.13.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.13.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.13.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.13.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.14.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.14.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.14.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.14.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.14.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.14.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.14.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.14.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.14.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.14.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.14.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.14.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.14.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.15.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.15.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.15.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.15.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.15.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.15.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.15.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.15.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.15.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.15.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.15.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.15.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.15.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.16.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.16.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.16.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.16.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.16.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.16.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.16.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.16.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.16.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.16.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.16.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.16.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.16.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.17.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.17.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.17.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.17.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.17.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.17.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.17.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.17.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.17.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.17.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.17.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.17.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.17.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.18.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.18.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.18.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.18.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.18.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.18.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.18.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.18.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.18.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.18.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.18.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.18.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.18.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.19.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.19.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.19.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.19.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.19.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.19.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.19.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.19.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.19.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.19.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.19.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.19.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.19.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.20.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.20.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.20.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.20.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.20.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.20.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.20.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.20.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.20.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.20.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.20.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.20.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.20.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.21.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.21.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.21.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.21.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.21.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.21.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.21.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.21.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.21.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.21.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.21.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.21.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.21.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.22.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.22.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.22.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.22.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.22.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.22.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.22.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.22.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.22.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.22.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.22.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.22.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.22.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.23.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.23.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.23.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.23.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.23.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.23.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.23.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.23.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.23.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.23.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.23.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.23.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.23.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.24.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.24.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.24.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.24.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.24.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.24.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.24.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.24.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.24.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.24.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.24.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.24.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.24.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.25.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.25.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.25.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.25.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.25.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.25.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.25.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.25.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.25.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.25.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.25.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.25.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.25.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.26.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.26.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.26.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.26.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.26.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.26.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.26.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.26.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.26.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.26.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.26.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.26.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.26.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.27.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.27.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.27.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.27.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.27.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.27.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.27.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.27.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.27.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.27.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.27.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.27.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.27.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.28.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.28.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.28.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.28.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.28.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.28.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.28.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.28.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.28.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.28.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.28.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.28.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.28.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.29.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.29.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.29.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.29.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.29.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.29.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.29.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.29.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.29.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.29.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.29.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.29.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.29.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.30.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.30.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.30.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.30.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.30.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.30.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.30.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.30.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.30.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.30.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.30.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.30.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.30.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.31.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.31.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.31.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.31.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.31.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.31.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.31.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.31.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.31.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.31.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.31.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.31.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.31.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.32.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.32.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.32.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.32.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.32.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.32.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.32.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.32.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.32.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.32.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.32.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.32.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.32.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.33.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.33.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.33.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.33.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.33.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.33.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.33.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.33.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.33.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.33.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.33.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.33.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.33.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.34.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.34.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.34.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.34.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.34.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.34.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.34.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.34.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.34.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.34.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.34.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.34.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.34.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.35.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.35.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.35.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.35.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.35.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.35.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.35.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.35.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.35.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.35.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.35.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.35.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.35.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.36.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.36.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.36.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.36.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.36.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.36.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.36.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.36.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.36.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.36.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.36.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.36.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.36.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.37.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.37.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.37.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.37.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.37.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.37.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.37.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.37.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.37.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.37.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.37.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.37.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.37.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.38.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.38.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.38.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.38.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.38.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.38.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.38.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.38.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.38.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.38.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.38.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.38.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.38.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.39.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.39.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.39.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.39.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.39.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.39.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.39.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.39.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.39.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.39.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.39.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.39.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.39.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.40.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.40.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.40.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.40.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.40.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.40.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.40.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.40.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.40.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.40.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.40.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.40.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.40.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.41.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.41.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.41.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.41.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.41.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.41.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.41.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.41.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.41.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.41.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.41.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.41.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.41.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.42.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.42.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.42.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.42.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.42.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.42.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.42.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.42.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.42.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.42.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.42.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.42.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.42.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.43.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.43.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.43.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.43.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.43.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.43.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.43.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.43.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.43.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.43.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.43.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.43.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.43.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.44.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.44.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.44.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.44.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.44.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.44.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.44.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.44.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.44.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.44.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.44.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.44.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.44.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.45.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.45.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.45.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.45.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.45.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.45.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.45.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.45.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.45.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.45.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.45.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.45.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.45.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.46.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.46.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.46.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.46.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.46.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.46.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.46.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.46.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.46.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.46.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.46.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.46.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.46.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.47.self_attn.q_proj.weight', 'gemma3_12b.transformer.model.layers.47.self_attn.k_proj.weight', 'gemma3_12b.transformer.model.layers.47.self_attn.v_proj.weight', 'gemma3_12b.transformer.model.layers.47.self_attn.o_proj.weight', 'gemma3_12b.transformer.model.layers.47.self_attn.q_norm.weight', 'gemma3_12b.transformer.model.layers.47.self_attn.k_norm.weight', 'gemma3_12b.transformer.model.layers.47.mlp.gate_proj.weight', 'gemma3_12b.transformer.model.layers.47.mlp.up_proj.weight', 'gemma3_12b.transformer.model.layers.47.mlp.down_proj.weight', 'gemma3_12b.transformer.model.layers.47.input_layernorm.weight', 'gemma3_12b.transformer.model.layers.47.post_attention_layernorm.weight', 'gemma3_12b.transformer.model.layers.47.pre_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.layers.47.post_feedforward_layernorm.weight', 'gemma3_12b.transformer.model.norm.weight', 'gemma3_12b.transformer.multi_modal_projector.mm_input_projection_weight', 'gemma3_12b.transformer.multi_modal_projector.mm_soft_emb_norm.weight', 'gemma3_12b.transformer.vision_model.embeddings.patch_embedding.weight', 'gemma3_12b.transformer.vision_model.embeddings.patch_embedding.bias', 'gemma3_12b.transformer.vision_model.embeddings.position_embedding.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.0.layer_norm1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.0.layer_norm1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.0.layer_norm2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.0.layer_norm2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.0.mlp.fc1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.0.mlp.fc1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.0.mlp.fc2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.0.mlp.fc2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.1.layer_norm1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.1.layer_norm1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.1.layer_norm2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.1.layer_norm2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.1.mlp.fc1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.1.mlp.fc1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.1.mlp.fc2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.1.mlp.fc2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.2.layer_norm1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.2.layer_norm1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.2.layer_norm2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.2.layer_norm2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.2.mlp.fc1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.2.mlp.fc1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.2.mlp.fc2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.2.mlp.fc2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.3.layer_norm1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.3.layer_norm1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.3.layer_norm2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.3.layer_norm2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.3.mlp.fc1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.3.mlp.fc1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.3.mlp.fc2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.3.mlp.fc2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.4.layer_norm1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.4.layer_norm1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.4.layer_norm2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.4.layer_norm2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.4.mlp.fc1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.4.mlp.fc1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.4.mlp.fc2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.4.mlp.fc2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.5.layer_norm1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.5.layer_norm1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.5.layer_norm2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.5.layer_norm2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.5.mlp.fc1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.5.mlp.fc1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.5.mlp.fc2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.5.mlp.fc2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.6.layer_norm1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.6.layer_norm1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.6.layer_norm2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.6.layer_norm2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.6.mlp.fc1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.6.mlp.fc1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.6.mlp.fc2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.6.mlp.fc2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.7.layer_norm1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.7.layer_norm1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.7.layer_norm2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.7.layer_norm2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.7.mlp.fc1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.7.mlp.fc1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.7.mlp.fc2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.7.mlp.fc2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.8.layer_norm1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.8.layer_norm1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.8.layer_norm2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.8.layer_norm2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.8.mlp.fc1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.8.mlp.fc1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.8.mlp.fc2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.8.mlp.fc2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.9.layer_norm1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.9.layer_norm1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.9.layer_norm2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.9.layer_norm2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.9.mlp.fc1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.9.mlp.fc1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.9.mlp.fc2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.9.mlp.fc2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.10.layer_norm1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.10.layer_norm1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.10.layer_norm2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.10.layer_norm2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.10.mlp.fc1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.10.mlp.fc1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.10.mlp.fc2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.10.mlp.fc2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.11.layer_norm1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.11.layer_norm1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.11.layer_norm2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.11.layer_norm2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.11.mlp.fc1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.11.mlp.fc1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.11.mlp.fc2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.11.mlp.fc2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.12.layer_norm1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.12.layer_norm1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.12.self_attn.q_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.12.self_attn.q_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.12.self_attn.k_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.12.self_attn.k_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.12.self_attn.v_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.12.self_attn.v_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.12.self_attn.out_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.12.self_attn.out_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.12.layer_norm2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.12.layer_norm2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.12.mlp.fc1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.12.mlp.fc1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.12.mlp.fc2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.12.mlp.fc2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.13.layer_norm1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.13.layer_norm1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.13.self_attn.q_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.13.self_attn.q_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.13.self_attn.k_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.13.self_attn.k_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.13.self_attn.v_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.13.self_attn.v_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.13.self_attn.out_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.13.self_attn.out_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.13.layer_norm2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.13.layer_norm2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.13.mlp.fc1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.13.mlp.fc1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.13.mlp.fc2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.13.mlp.fc2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.14.layer_norm1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.14.layer_norm1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.14.self_attn.q_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.14.self_attn.q_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.14.self_attn.k_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.14.self_attn.k_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.14.self_attn.v_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.14.self_attn.v_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.14.self_attn.out_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.14.self_attn.out_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.14.layer_norm2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.14.layer_norm2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.14.mlp.fc1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.14.mlp.fc1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.14.mlp.fc2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.14.mlp.fc2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.15.layer_norm1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.15.layer_norm1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.15.self_attn.q_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.15.self_attn.q_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.15.self_attn.k_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.15.self_attn.k_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.15.self_attn.v_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.15.self_attn.v_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.15.self_attn.out_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.15.self_attn.out_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.15.layer_norm2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.15.layer_norm2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.15.mlp.fc1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.15.mlp.fc1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.15.mlp.fc2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.15.mlp.fc2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.16.layer_norm1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.16.layer_norm1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.16.self_attn.q_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.16.self_attn.q_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.16.self_attn.k_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.16.self_attn.k_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.16.self_attn.v_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.16.self_attn.v_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.16.self_attn.out_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.16.self_attn.out_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.16.layer_norm2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.16.layer_norm2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.16.mlp.fc1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.16.mlp.fc1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.16.mlp.fc2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.16.mlp.fc2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.17.layer_norm1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.17.layer_norm1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.17.self_attn.q_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.17.self_attn.q_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.17.self_attn.k_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.17.self_attn.k_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.17.self_attn.v_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.17.self_attn.v_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.17.self_attn.out_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.17.self_attn.out_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.17.layer_norm2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.17.layer_norm2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.17.mlp.fc1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.17.mlp.fc1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.17.mlp.fc2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.17.mlp.fc2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.18.layer_norm1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.18.layer_norm1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.18.self_attn.q_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.18.self_attn.q_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.18.self_attn.k_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.18.self_attn.k_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.18.self_attn.v_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.18.self_attn.v_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.18.self_attn.out_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.18.self_attn.out_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.18.layer_norm2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.18.layer_norm2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.18.mlp.fc1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.18.mlp.fc1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.18.mlp.fc2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.18.mlp.fc2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.19.layer_norm1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.19.layer_norm1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.19.self_attn.q_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.19.self_attn.q_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.19.self_attn.k_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.19.self_attn.k_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.19.self_attn.v_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.19.self_attn.v_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.19.self_attn.out_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.19.self_attn.out_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.19.layer_norm2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.19.layer_norm2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.19.mlp.fc1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.19.mlp.fc1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.19.mlp.fc2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.19.mlp.fc2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.20.layer_norm1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.20.layer_norm1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.20.self_attn.q_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.20.self_attn.q_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.20.self_attn.k_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.20.self_attn.k_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.20.self_attn.v_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.20.self_attn.v_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.20.self_attn.out_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.20.self_attn.out_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.20.layer_norm2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.20.layer_norm2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.20.mlp.fc1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.20.mlp.fc1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.20.mlp.fc2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.20.mlp.fc2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.21.layer_norm1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.21.layer_norm1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.21.self_attn.q_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.21.self_attn.q_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.21.self_attn.k_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.21.self_attn.k_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.21.self_attn.v_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.21.self_attn.v_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.21.self_attn.out_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.21.self_attn.out_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.21.layer_norm2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.21.layer_norm2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.21.mlp.fc1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.21.mlp.fc1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.21.mlp.fc2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.21.mlp.fc2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.22.layer_norm1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.22.layer_norm1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.22.self_attn.q_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.22.self_attn.q_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.22.self_attn.k_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.22.self_attn.k_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.22.self_attn.v_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.22.self_attn.v_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.22.self_attn.out_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.22.self_attn.out_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.22.layer_norm2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.22.layer_norm2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.22.mlp.fc1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.22.mlp.fc1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.22.mlp.fc2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.22.mlp.fc2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.23.layer_norm1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.23.layer_norm1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.23.self_attn.q_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.23.self_attn.q_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.23.self_attn.k_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.23.self_attn.k_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.23.self_attn.v_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.23.self_attn.v_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.23.self_attn.out_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.23.self_attn.out_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.23.layer_norm2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.23.layer_norm2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.23.mlp.fc1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.23.mlp.fc1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.23.mlp.fc2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.23.mlp.fc2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.24.layer_norm1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.24.layer_norm1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.24.self_attn.q_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.24.self_attn.q_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.24.self_attn.k_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.24.self_attn.k_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.24.self_attn.v_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.24.self_attn.v_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.24.self_attn.out_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.24.self_attn.out_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.24.layer_norm2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.24.layer_norm2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.24.mlp.fc1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.24.mlp.fc1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.24.mlp.fc2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.24.mlp.fc2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.25.layer_norm1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.25.layer_norm1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.25.self_attn.q_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.25.self_attn.q_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.25.self_attn.k_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.25.self_attn.k_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.25.self_attn.v_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.25.self_attn.v_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.25.self_attn.out_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.25.self_attn.out_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.25.layer_norm2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.25.layer_norm2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.25.mlp.fc1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.25.mlp.fc1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.25.mlp.fc2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.25.mlp.fc2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.26.layer_norm1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.26.layer_norm1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.26.self_attn.q_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.26.self_attn.q_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.26.self_attn.k_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.26.self_attn.k_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.26.self_attn.v_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.26.self_attn.v_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.26.self_attn.out_proj.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.26.self_attn.out_proj.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.26.layer_norm2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.26.layer_norm2.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.26.mlp.fc1.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.26.mlp.fc1.bias', 'gemma3_12b.transformer.vision_model.encoder.layers.26.mlp.fc2.weight', 'gemma3_12b.transformer.vision_model.encoder.layers.26.mlp.fc2.bias', 'gemma3_12b.transformer.vision_model.post_layernorm.weight', 'gemma3_12b.transformer.vision_model.post_layernorm.bias']
CLIP/text encoder model load device: cuda:0, offload device: cpu, current: cpu, dtype: torch.float16
Requested to load LTXAVTEModel_
loaded partially; 9562.80 MB usable, 735.62 MB loaded, 25228.59 MB offloaded, 8827.18 MB buffer reserved, lowvram patches: 0
FETCH ComfyRegistry Data: 20/120
FETCH ComfyRegistry Data: 25/120
gguf qtypes: F32 (2140), BF16 (26), Q4_K (1008), Q6_K (336)
model weight dtype torch.bfloat16, manual cast: None
model_type FLUX
Requested to load LTXAV
FETCH ComfyRegistry Data: 30/120
FETCH ComfyRegistry Data: 35/120
FETCH ComfyRegistry Data: 40/120
loaded partially; 9541.56 MB usable, 9388.43 MB loaded, 2853.54 MB offloaded, 153.13 MB buffer reserved, lowvram patches: 0
  0%|          | 0/8 [00:00<?, ?it/s]FETCH ComfyRegistry Data: 45/120
FETCH ComfyRegistry Data: 50/120
FETCH ComfyRegistry Data: 55/120
 12%|‚ñà‚ñé        | 1/8 [00:19<02:17, 19.62s/it]FETCH ComfyRegistry Data: 60/120
FETCH ComfyRegistry Data: 65/120
 25%|‚ñà‚ñà‚ñå       | 2/8 [00:30<01:28, 14.71s/it]FETCH ComfyRegistry Data: 70/120
FETCH ComfyRegistry Data: 75/120
 38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:41<01:05, 13.01s/it]FETCH ComfyRegistry Data: 80/120
FETCH ComfyRegistry Data: 85/120
 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:52<00:48, 12.15s/it]FETCH ComfyRegistry Data: 90/120
FETCH ComfyRegistry Data: 95/120
 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [01:03<00:34, 11.66s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [01:14<00:22, 11.39s/it]FETCH ComfyRegistry Data: 100/120
FETCH ComfyRegistry Data: 105/120
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [01:25<00:11, 11.21s/it]FETCH ComfyRegistry Data: 110/120
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [01:36<00:00, 11.11s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [01:36<00:00, 12.01s/it]
FETCH ComfyRegistry Data: 115/120
Requested to load VideoVAE
Unloaded partially: 2398.64 MB freed, 6989.78 MB remains loaded, 177.49 MB buffer reserved, lowvram patches: 211
loaded completely; 2648.72 MB usable, 2378.23 MB loaded, full load: True
loaded partially; 9227.11 MB usable, 9073.58 MB loaded, 3168.39 MB offloaded, 153.53 MB buffer reserved, lowvram patches: 0
  0%|          | 0/3 [00:00<?, ?it/s]FETCH ComfyRegistry Data: 120/120
ERROR lora diffusion_model.transformer_blocks.0.ff.net.2.weight Allocation on device 
ERROR lora diffusion_model.transformer_blocks.0.ff.net.2.weight Allocation on device 
ERROR lora diffusion_model.transformer_blocks.0.ff.net.2.weight Allocation on device 
ERROR lora diffusion_model.transformer_blocks.0.ff.net.2.weight Allocation on device 
FETCH ComfyRegistry Data [DONE]
[ComfyUI-Manager] default cache updated: https://api.comfy.org/nodes
FETCH DATA from: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/custom-node-list.json [DONE]
[ComfyUI-Manager] All startup tasks have been completed.
 33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:22<00:44, 22.33s/it] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:44<00:22, 22.31s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [01:07<00:00, 22.38s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [01:07<00:00, 22.37s/it]
Requested to load VideoVAE
loaded partially; 564.00 MB usable, 0.00 MB loaded, 2378.23 MB offloaded, 1728.05 MB buffer reserved, lowvram patches: 0
Requested to load AudioVAE
loaded completely; 9541.67 MB usable, 415.20 MB loaded, full load: True
Prompt executed in 237.74 seconds
‚úÖ Copied 73 frames for scene_1.1_1
Saved video: ../output\scene_1.1_1.mp4
üíæ Checkpoint saved for chunk: scene_1.1_1
Generating chunk: scene_1.1_2 (73 frames)
üîç Frame prefix: scene_1.1_1.frame_
üîç Matching frames: 73
üîç Found last frame: scene_1.1_1.frame_073.png (frame #73)
üîÑ Using last saved frame from previous chunk as input for scene_1.1_2
üìÅ Frame stored: ../output/frames\scene_1.1_2_last_frame.png
üí¨ Chunk 2 dialogue (3.04s, 15 words): a deceptive tranquility, and as I sat with my news...
üé¨ Using master prompt for 1.1
üí¨ Added dialogue chunk to prompt: a deceptive tranquility, and as I sat with my news...
Full prompt: Style: anime - cinematic - The young man sits on a blue couch near a window, reading a newspaper. Sunlight streams through the glass, illuminating his face and casting soft shadows across the room. He wears a white short-sleeved shirt and brown pants, with a watch on his left wrist. A small wooden table stands beside him, while green plants grow in the corner of the room. On the wall behind him hangs a framed picture. Outside the window, trees sway gently under a bright blue sky dotted with fluffy clouds. The sound of birds chirping fills the air, accompanied by the soft rustling of leaves and distant traffic noise from the street below.

Dialogue: "a deceptive tranquility, and as I sat with my newspaper I little suspected that a"
got prompt
Waiting for video generation completion (prompt_id: a8a5c92c-c597-4402-b34d-7de51ed9f9a9)...
loaded completely; 9126.48 MB usable, 2378.23 MB loaded, full load: True
Requested to load LTXAVTEModel_
loaded partially; 9541.67 MB usable, 714.50 MB loaded, 25249.59 MB offloaded, 8827.18 MB buffer reserved, lowvram patches: 0
Requested to load LTXAV
loaded partially; 9539.56 MB usable, 9386.43 MB loaded, 2855.54 MB offloaded, 153.13 MB buffer reserved, lowvram patches: 0
  0%|          | 0/8 [00:00<?, ?it/s] 12%|‚ñà‚ñé        | 1/8 [00:11<01:18, 11.22s/it] 25%|‚ñà‚ñà‚ñå       | 2/8 [00:22<01:06, 11.05s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:33<00:54, 10.98s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:43<00:43, 10.96s/it] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:54<00:32, 10.94s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [01:05<00:21, 10.91s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [01:16<00:10, 10.93s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [01:27<00:00, 10.98s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [01:27<00:00, 10.97s/it]
Requested to load VideoVAE
Unloaded partially: 2396.64 MB freed, 6989.78 MB remains loaded, 177.49 MB buffer reserved, lowvram patches: 211
loaded completely; 2648.72 MB usable, 2378.23 MB loaded, full load: True
loaded partially; 9227.11 MB usable, 9073.58 MB loaded, 3168.39 MB offloaded, 153.53 MB buffer reserved, lowvram patches: 0
  0%|          | 0/3 [00:00<?, ?it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:22<00:45, 22.55s/it] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:45<00:22, 22.54s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [01:07<00:00, 22.67s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [01:07<00:00, 22.63s/it]
Requested to load VideoVAE
loaded partially; 564.00 MB usable, 0.00 MB loaded, 2378.23 MB offloaded, 1728.05 MB buffer reserved, lowvram patches: 0
Requested to load AudioVAE
loaded completely; 9541.67 MB usable, 415.20 MB loaded, full load: True
Prompt executed in 197.22 seconds
‚úÖ Copied 73 frames for scene_1.1_2
Saved video: ../output\scene_1.1_2.mp4
üíæ Checkpoint saved for chunk: scene_1.1_2
Generating chunk: scene_1.1_3 (70 frames)
üîç Frame prefix: scene_1.1_2.frame_
üîç Matching frames: 73
üîç Found last frame: scene_1.1_2.frame_073.png (frame #73)
üîÑ Using last saved frame from previous chunk as input for scene_1.1_3
üìÅ Frame stored: ../output/frames\scene_1.1_3_last_frame.png
üí¨ Chunk 3 dialogue (2.92s, 15 words): most singular case, involving an absurdly colored ...
üé¨ Using master prompt for 1.1
üí¨ Added dialogue chunk to prompt: most singular case, involving an absurdly colored ...
Full prompt: Style: anime - cinematic - The young man sits on a blue couch near a window, reading a newspaper. Sunlight streams through the glass, illuminating his face and casting soft shadows across the room. He wears a white short-sleeved shirt and brown pants, with a watch on his left wrist. A small wooden table stands beside him, while green plants grow in the corner of the room. On the wall behind him hangs a framed picture. Outside the window, trees sway gently under a bright blue sky dotted with fluffy clouds. The sound of birds chirping fills the air, accompanied by the soft rustling of leaves and distant traffic noise from the street below.

Dialogue: "most singular case, involving an absurdly colored duckling, was about to disturb our domestic peace.";"
got prompt
Waiting for video generation completion (prompt_id: 84f35493-6a1e-4c6c-8f9e-8338347e9e1a)...
loaded completely; 9126.48 MB usable, 2378.23 MB loaded, full load: True
Requested to load LTXAVTEModel_
loaded partially; 9541.67 MB usable, 714.50 MB loaded, 25249.59 MB offloaded, 8827.18 MB buffer reserved, lowvram patches: 0
Requested to load LTXAV
loaded partially; 9539.77 MB usable, 9386.63 MB loaded, 2855.34 MB offloaded, 153.13 MB buffer reserved, lowvram patches: 0
  0%|          | 0/8 [00:00<?, ?it/s] 12%|‚ñà‚ñé        | 1/8 [00:11<01:17, 11.03s/it] 25%|‚ñà‚ñà‚ñå       | 2/8 [00:21<01:04, 10.76s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:32<00:53, 10.69s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:42<00:42, 10.69s/it] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:53<00:31, 10.66s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [01:04<00:21, 10.63s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [01:14<00:10, 10.62s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [01:25<00:00, 10.61s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [01:25<00:00, 10.66s/it]
Requested to load VideoVAE
Unloaded partially: 2396.85 MB freed, 6989.78 MB remains loaded, 177.49 MB buffer reserved, lowvram patches: 211
loaded completely; 2648.72 MB usable, 2378.23 MB loaded, full load: True
loaded partially; 9359.11 MB usable, 9205.62 MB loaded, 3036.35 MB offloaded, 153.48 MB buffer reserved, lowvram patches: 0
  0%|          | 0/3 [00:00<?, ?it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:20<00:41, 20.86s/it] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:41<00:20, 20.88s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [01:02<00:00, 20.74s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [01:02<00:00, 20.78s/it]
Requested to load VideoVAE
loaded partially; 1543.69 MB usable, 0.00 MB loaded, 2378.23 MB offloaded, 1728.05 MB buffer reserved, lowvram patches: 0
Requested to load AudioVAE
loaded completely; 9541.67 MB usable, 415.20 MB loaded, full load: True
Prompt executed in 190.19 seconds
‚úÖ Copied 65 frames for scene_1.1_3
Saved video: ../output\scene_1.1_3.mp4
üíæ Checkpoint saved for chunk: scene_1.1_3
üîó Merging 3 video chunks for scene_1.1...
Merging 3 video chunks with audio...
‚úÖ Successfully merged chunks with audio: scene_1.1.mp4
üóëÔ∏è Cleaned up chunk: scene_1.1_1.mp4
üóëÔ∏è Cleaned up chunk: scene_1.1_2.mp4
üóëÔ∏è Cleaned up chunk: scene_1.1_3.mp4
‚úÖ Scene 1/7 - "It was a calm winter morning in Baker Street, the (9.00s) - Completed in 10.7m
üìä Estimated time remaining: ~56.1m (?)
‚úÖ Generated video: scene_1.1
üîÑ Scene 2/7 - "You mistake calm for emptiness, Watson, for I per (7.60s) - Processing...
üìä Estimated time remaining: ~53.3m (?)
Generating video for scene: scene_1.2
Duration: 7.60s, Frames: 182
Video chunks: 3
Copied scene image to ComfyUI input: ../../ComfyUI/input\scene_1.2.png
Generating chunk: scene_1.2_1 (73 frames)
üí¨ Chunk 1 dialogue (3.04s, 15 words): "You mistake calm for emptiness, Watson, for I per...
üé¨ Using master prompt for 1.2
üí¨ Added dialogue chunk to prompt: "You mistake calm for emptiness, Watson, for I per...
Full prompt: Style: anime - realistic - cinematic - The man with black hair and blue eyes stands near the window, his finger pointing at the sill. He speaks in a calm, authoritative voice saying: "Observe, Watson, our nocturnal visitor has been most careless. The marks upon this sill tell me she stood here for some moments before deciding to knock." In the background, the man with dark hair and blue eyes leans forward from the armchair, his newspaper lowered as he listens intently. A fire burns warmly in the fireplace, casting a soft orange glow across the room. Outside, green trees sway gently under a clear blue sky. The sound of crackling wood and distant birdsong blend into a quiet, contemplative atmosphere.

Dialogue: ""You mistake calm for emptiness, Watson, for I perceive from the stiffness of your posture"
got prompt
Waiting for video generation completion (prompt_id: c09495fd-8630-4d6c-a9e6-7d92ad3962ee)...
loaded completely; 9126.48 MB usable, 2378.23 MB loaded, full load: True
Requested to load LTXAVTEModel_
loaded partially; 9541.67 MB usable, 714.50 MB loaded, 25249.59 MB offloaded, 8827.18 MB buffer reserved, lowvram patches: 0
Requested to load LTXAV
loaded partially; 9539.56 MB usable, 9386.43 MB loaded, 2855.54 MB offloaded, 153.13 MB buffer reserved, lowvram patches: 0
  0%|          | 0/8 [00:00<?, ?it/s] 12%|‚ñà‚ñé        | 1/8 [00:11<01:21, 11.69s/it] 25%|‚ñà‚ñà‚ñå       | 2/8 [00:22<01:08, 11.36s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:33<00:56, 11.21s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:44<00:44, 11.17s/it] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:56<00:33, 11.17s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [01:07<00:22, 11.16s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [01:18<00:11, 11.15s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [01:29<00:00, 11.08s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [01:29<00:00, 11.17s/it]
Requested to load VideoVAE
Unloaded partially: 2396.64 MB freed, 6989.78 MB remains loaded, 177.49 MB buffer reserved, lowvram patches: 211
loaded completely; 2648.72 MB usable, 2378.23 MB loaded, full load: True
loaded partially; 9227.11 MB usable, 9073.58 MB loaded, 3168.39 MB offloaded, 153.53 MB buffer reserved, lowvram patches: 0
  0%|          | 0/3 [00:00<?, ?it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:22<00:44, 22.27s/it] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:44<00:22, 22.27s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [01:06<00:00, 22.21s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [01:06<00:00, 22.22s/it]
Requested to load VideoVAE
loaded partially; 564.00 MB usable, 0.00 MB loaded, 2378.23 MB offloaded, 1728.05 MB buffer reserved, lowvram patches: 0
Requested to load AudioVAE
loaded completely; 9541.67 MB usable, 415.20 MB loaded, full load: True
Prompt executed in 203.59 seconds
‚úÖ Copied 73 frames for scene_1.2_1
Saved video: ../output\scene_1.2_1.mp4
üíæ Checkpoint saved for chunk: scene_1.2_1
Generating chunk: scene_1.2_2 (73 frames)
üîç Frame prefix: scene_1.2_1.frame_
üîç Matching frames: 73
üîç Found last frame: scene_1.2_1.frame_073.png (frame #73)
üîÑ Using last saved frame from previous chunk as input for scene_1.2_2
üìÅ Frame stored: ../output/frames\scene_1.2_2_last_frame.png
üí¨ Chunk 2 dialogue (3.04s, 15 words): and the untouched tea that your mind is already wa...
üé¨ Using master prompt for 1.2
üí¨ Added dialogue chunk to prompt: and the untouched tea that your mind is already wa...
Full prompt: Style: anime - realistic - cinematic - The man with black hair and blue eyes stands near the window, his finger pointing at the sill. He speaks in a calm, authoritative voice saying: "Observe, Watson, our nocturnal visitor has been most careless. The marks upon this sill tell me she stood here for some moments before deciding to knock." In the background, the man with dark hair and blue eyes leans forward from the armchair, his newspaper lowered as he listens intently. A fire burns warmly in the fireplace, casting a soft orange glow across the room. Outside, green trees sway gently under a clear blue sky. The sound of crackling wood and distant birdsong blend into a quiet, contemplative atmosphere.

Dialogue: "and the untouched tea that your mind is already wandering, which suggests that you sense,"
got prompt
Waiting for video generation completion (prompt_id: f9b2b007-76e5-4602-a4ab-febdc5fef01f)...
loaded completely; 9126.48 MB usable, 2378.23 MB loaded, full load: True
Requested to load LTXAVTEModel_
loaded partially; 9541.67 MB usable, 714.50 MB loaded, 25249.59 MB offloaded, 8827.18 MB buffer reserved, lowvram patches: 0
Requested to load LTXAV
loaded partially; 9539.56 MB usable, 9386.43 MB loaded, 2855.54 MB offloaded, 153.13 MB buffer reserved, lowvram patches: 0
  0%|          | 0/8 [00:00<?, ?it/s] 12%|‚ñà‚ñé        | 1/8 [00:11<01:21, 11.66s/it] 25%|‚ñà‚ñà‚ñå       | 2/8 [00:22<01:08, 11.35s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:34<00:56, 11.29s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:45<00:44, 11.20s/it] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:56<00:33, 11.14s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [01:07<00:22, 11.13s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [01:18<00:11, 11.11s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [01:29<00:00, 11.01s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [01:29<00:00, 11.13s/it]
Requested to load VideoVAE
Unloaded partially: 2396.64 MB freed, 6989.78 MB remains loaded, 177.49 MB buffer reserved, lowvram patches: 211
loaded completely; 2648.72 MB usable, 2378.23 MB loaded, full load: True
loaded partially; 9227.11 MB usable, 9073.58 MB loaded, 3168.39 MB offloaded, 153.53 MB buffer reserved, lowvram patches: 0
  0%|          | 0/3 [00:00<?, ?it/s]ERROR lora diffusion_model.transformer_blocks.0.ff.net.0.proj.weight Allocation on device 
ERROR lora diffusion_model.transformer_blocks.0.ff.net.2.weight Allocation on device 
ERROR lora diffusion_model.transformer_blocks.0.ff.net.2.weight Allocation on device 
ERROR lora diffusion_model.transformer_blocks.0.ff.net.2.weight Allocation on device 
ERROR lora diffusion_model.transformer_blocks.0.ff.net.2.weight Allocation on device 
 33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:23<00:47, 23.92s/it] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:46<00:22, 22.94s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [01:29<00:00, 32.46s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [01:29<00:00, 29.99s/it]
Requested to load VideoVAE
loaded partially; 564.00 MB usable, 0.00 MB loaded, 2378.23 MB offloaded, 1728.05 MB buffer reserved, lowvram patches: 0
Requested to load AudioVAE
loaded completely; 9541.67 MB usable, 415.20 MB loaded, full load: True
Prompt executed in 224.66 seconds
‚úÖ Copied 73 frames for scene_1.2_2
Saved video: ../output\scene_1.2_2.mp4
üíæ Checkpoint saved for chunk: scene_1.2_2
Generating chunk: scene_1.2_3 (36 frames)
üîç Frame prefix: scene_1.2_2.frame_
üîç Matching frames: 73
üîç Found last frame: scene_1.2_2.frame_073.png (frame #73)
üîÑ Using last saved frame from previous chunk as input for scene_1.2_3
üìÅ Frame stored: ../output/frames\scene_1.2_3_last_frame.png
üí¨ Chunk 3 dialogue (1.50s, 8 words): however dimly, the approach of something decidedly...
üé¨ Using master prompt for 1.2
üí¨ Added dialogue chunk to prompt: however dimly, the approach of something decidedly...
Full prompt: Style: anime - realistic - cinematic - The man with black hair and blue eyes stands near the window, his finger pointing at the sill. He speaks in a calm, authoritative voice saying: "Observe, Watson, our nocturnal visitor has been most careless. The marks upon this sill tell me she stood here for some moments before deciding to knock." In the background, the man with dark hair and blue eyes leans forward from the armchair, his newspaper lowered as he listens intently. A fire burns warmly in the fireplace, casting a soft orange glow across the room. Outside, green trees sway gently under a clear blue sky. The sound of crackling wood and distant birdsong blend into a quiet, contemplative atmosphere.

Dialogue: "however dimly, the approach of something decidedly unusual.";"
got prompt
Waiting for video generation completion (prompt_id: c7454be5-39e8-4e4c-9b5c-fea99d669ed9)...
loaded completely; 9126.48 MB usable, 2378.23 MB loaded, full load: True
Requested to load LTXAVTEModel_
loaded partially; 9541.67 MB usable, 714.50 MB loaded, 25249.59 MB offloaded, 8827.18 MB buffer reserved, lowvram patches: 0
Requested to load LTXAV
loaded partially; 9540.62 MB usable, 9387.48 MB loaded, 2854.49 MB offloaded, 153.13 MB buffer reserved, lowvram patches: 0
  0%|          | 0/8 [00:00<?, ?it/s] 12%|‚ñà‚ñé        | 1/8 [00:10<01:12, 10.37s/it] 25%|‚ñà‚ñà‚ñå       | 2/8 [00:19<00:58,  9.83s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:29<00:48,  9.67s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:38<00:38,  9.59s/it] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:48<00:28,  9.49s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [00:57<00:19,  9.54s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [01:07<00:09,  9.67s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [01:17<00:00,  9.76s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [01:17<00:00,  9.70s/it]
Requested to load VideoVAE
Unloaded partially: 2397.70 MB freed, 6989.78 MB remains loaded, 177.49 MB buffer reserved, lowvram patches: 211
loaded completely; 2644.72 MB usable, 2378.23 MB loaded, full load: True
loaded partially; 9725.56 MB usable, 9572.42 MB loaded, 2669.55 MB offloaded, 153.13 MB buffer reserved, lowvram patches: 0
  0%|          | 0/3 [00:00<?, ?it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:14<00:29, 14.99s/it] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:29<00:14, 14.66s/it]Traceback (most recent call last):
  File "D:\.comfyui\gen.av\scripts\2.av.py", line 1479, in <module>
    cached_result['needs_merge'] = False

  File "D:\.comfyui\gen.av\scripts\2.av.py", line 1453, in main
    for j, path in enumerate(output_paths, 1):
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\.comfyui\gen.av\scripts\2.av.py", line 1382, in generate_all_videos
    # Will check if chunks exist and regenerate missing ones if needed
   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\.comfyui\gen.av\scripts\2.av.py", line 1071, in _generate_video
    print(f"ERROR: ComfyUI API error: {resp.status_code} {resp.text}")
    ^^^^^^^^^^^^^
KeyboardInterrupt
Received signal 2. Cleaning up services...
Stopping ComfyUI backend...
ComfyUI still responding; waiting for shutdown...
ComfyUI appears stopped (after 2 checks).
